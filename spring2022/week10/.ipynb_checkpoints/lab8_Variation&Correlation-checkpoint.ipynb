{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 8: Variance of Sample Means and Correlation\n",
    "\n",
    "In this lab we will learn about [the variance of sample means](https://inferentialthinking.com/chapters/14/5/Variability_of_the_Sample_Mean.html) as well as ways to understand and quantify [the association between two variables](https://inferentialthinking.com/chapters/15/1/Correlation.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import statements\n",
    "try:\n",
    "    # These lines load the tests. \n",
    "    from gofer.ok import check\n",
    "    \n",
    "except:\n",
    "    # Install a pip package in the current Jupyter kernel\n",
    "    import sys\n",
    "    !{sys.executable} -m pip install git+https://github.com/grading/gradememaybe.git\n",
    "\n",
    "    # These lines load the tests. \n",
    "    from gofer.ok import check\n",
    "import numpy as np\n",
    "from datascience import *\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. How Faithful is Old Faithful? \n",
    "\n",
    "(Note: clever title comes from [here](http://web.pdx.edu/~jfreder/M212/oldfaithful.pdf).)\n",
    "\n",
    "Old Faithful is a geyser in Yellowstone National Park in the central United States.  It's famous for erupting on a fairly regular schedule.  You can see a video below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the curious: this is how to display a YouTube video in a\n",
    "# Jupyter notebook.  The argument to YouTubeVideo is the part\n",
    "# of the URL (called a \"query parameter\") that identifies the\n",
    "# video.  For example, the full URL for this video is:\n",
    "#   https://www.youtube.com/watch?v=wE8NDuzt8eg\n",
    "from IPython.display import YouTubeVideo\n",
    "YouTubeVideo(\"wE8NDuzt8eg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of Old Faithful's eruptions last longer than others.  When it has a long eruption, there's generally a longer wait until the next eruption.\n",
    "\n",
    "If you visit Yellowstone, you might want to predict when the next eruption will happen, so you can see the rest of the park and come to see the geyser when it erupts.  To predict one variable from another, the first step is to understand the association between them.\n",
    "\n",
    "The dataset has one row for each observed eruption.  It includes the following columns:\n",
    "- **duration**: Eruption duration, in minutes\n",
    "- **wait**: Time between this eruption and the next, also in minutes\n",
    "\n",
    "Run the next cell to load the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faithful = Table.read_table(\"faithful.csv\")\n",
    "faithful"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1**\n",
    "<br/>\n",
    "Make a scatter plot of the data.  It's conventional to put the column we will try to predict on the vertical axis and the other column on the horizontal axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_col = ...\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check('tests/q1.py')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2**\n",
    "\n",
    "Look at the scatter plot. Are eruption duration and waiting time roughly linearly related?  Is the relationship positive, as we claimed earlier?  You may want to consult [the textbook chapter 15](https://inferentialthinking.com/chapters/15/1/Correlation.html#the-correlation-coefficient) for the definition of \"linearly related.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Write your answer here, replacing this text.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The eruption durations seem to cluster; there are a bunch of short eruptions and a bunch of longer ones.  Within each of the clusters, these values appear to be roughly linearly correlated, but perhaps with a different correlation coefficient.\n",
    "\n",
    "The overall relationship is positive, which means that longer eruptions have longer waiting times. Even when the association is more nuanced than a simple linear association, we can still compute the correlation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll plot the data in standard units.  Recall that, if `nums` is an array of numbers, then\n",
    "\n",
    "    (nums - np.mean(nums)) / np.std(nums)\n",
    "\n",
    "is an array of those numbers in standard units."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3**\n",
    "<br/>\n",
    "Compute the mean and standard deviation of the eruption durations and waiting times.  **Then** create a table called `faithful_standard` containing the eruption durations and waiting times in standard units.  (The columns should be named `\"duration (standard units)\"` and `\"wait (standard units)\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "for_assignment_type": "student"
   },
   "outputs": [],
   "source": [
    "duration_mean = ...\n",
    "duration_std = ...\n",
    "wait_mean = ...\n",
    "wait_std = ...\n",
    "\n",
    "faithful_standard = Table().with_columns(\n",
    "    \"duration (standard units)\", ...,\n",
    "    \"wait (standard units)\", ...)\n",
    "faithful_standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check('tests/q3.py')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4**\n",
    "<br/>\n",
    "Plot the data again, but this time in standard units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll notice that this plot looks exactly the same as the last one!  The data really are different, but the axes are scaled differently.  (The method `scatter` scales the axes so the data fill up the available space.)  So it's important to read the ticks on the axes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 5**\n",
    "\n",
    "Among the following numbers, which would you guess is closest to the correlation between eruption duration and waiting time in this dataset?\n",
    "\n",
    "* -1\n",
    "* 0\n",
    "* 1\n",
    "\n",
    "Assign your answer to `closest_correlation`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "closest_correlation = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check('tests/q5.py')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 6**\n",
    "<br/>\n",
    "Compute the correlation `r`.  *Hint:* Use `faithful_standard`.  Section [15.1.2](https://inferentialthinking.com/chapters/15/1/Correlation.html#calculating-r) explains how to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = ...\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check('tests/q6.py')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The regression line\n",
    "Recall that the correlation is the slope of the regression line when the data are put in standard units.\n",
    "\n",
    "The next cell plots the regression line in standard units:\n",
    "\n",
    "$$\\text{waiting time (standard units)} = r \\times \\text{eruption duration (standard units)}.$$\n",
    "\n",
    "Then, it plots the original data again, for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data_and_line(dataset, x, y, point_0, point_1):\n",
    "    \"\"\"Makes a scatter plot of the dataset, along with a line passing through two points.\"\"\"\n",
    "    dataset.scatter(x, y, label=\"data\")\n",
    "    xs, ys = zip(point_0, point_1)\n",
    "    plt.plot(xs, ys, label=\"regression line\")\n",
    "    plt.legend(bbox_to_anchor=(1.5,.8))\n",
    "\n",
    "plot_data_and_line(faithful_standard, \n",
    "                   \"duration (standard units)\", \n",
    "                   \"wait (standard units)\", \n",
    "                   [-2, -2*r], \n",
    "                   [2, 2*r])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How would you take a point in standard units and convert it back to original units?  We'd have to \"stretch\" its horizontal position by `duration_std` and its vertical position by `wait_std`.\n",
    "\n",
    "That means the same thing would happen to the slope of the line.\n",
    "\n",
    "Stretching a line horizontally makes it less steep, so we divide the slope by the stretching factor.  Stretching a line vertically makes it more steep, so we multiply the slope by the stretching factor.\n",
    "\n",
    "#### Question 7\n",
    "What is the slope of the regression line in original units?\n",
    "\n",
    "(If the \"stretching\" explanation is unintuitive, consult section [15.2.5](https://inferentialthinking.com/chapters/15/2/Regression_Line.html#the-equation-of-the-regression-line) in the textbook.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slope = ...\n",
    "slope"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that the regression line passes through the point `(duration_mean, wait_mean)`.  You might recall from high-school algebra that the equation for the line is therefore:\n",
    "\n",
    "$$\\text{waiting time} - \\verb|wait_mean| = \\texttt{slope} \\times (\\text{eruption duration} - \\verb|duration_mean|)$$\n",
    "\n",
    "After rearranging that equation slightly, the intercept turns out to be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intercept = slope*(-duration_mean) + wait_mean\n",
    "intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check('tests/q7.py')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Variability of the Sample Mean\n",
    "\n",
    "By the Central Limit Theorem, the probability distribution of the mean of a large random sample is roughly normal. The bell curve is centered at the population mean. Some of the sample means are higher, and some lower, but the deviations from the population mean are roughly symmetric on either side, as we have seen repeatedly. Formally, probability theory shows that the sample mean is an unbiased estimate of the population mean.\n",
    "\n",
    "In our simulations, we also noticed that the means of larger samples tend to be more tightly clustered around the population mean than means of smaller samples. In this section, we will quantify the variability of the sample mean and develop a relation between the variability and the sample size.\n",
    "\n",
    "Let's take a look at the salaries of employees of the City of San Francisco in 2014. The mean salary reported by the city government was about $75463.92."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "salaries = Table.read_table('sf_salaries_2014.csv').select(\"salary\")\n",
    "salaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "salary_mean = np.mean(salaries.column('salary'))\n",
    "salary_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "salaries.hist('salary', bins=np.arange(0, 300000+10000*2, 10000))\n",
    "plt.scatter(salary_mean, 0, marker='^', color='red', s=1000);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 8**\n",
    "<br/>\n",
    "Clearly, the population does not follow a normal distribution. Keep that in mind as we progress through these exercises.\n",
    "\n",
    "Let's take random samples and look at the probability distribution of the sample mean. As usual, we will use simulation to get an empirical approximation to this distribution.\n",
    "\n",
    "We will define a function `simulate_sample_mean` to do this, because we are going to vary the sample size later. The arguments are the name of the table, the label of the column containing the variable, the sample size, and the number of simulations.\n",
    "\n",
    "Complete the function `simulate_sample_mean`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Empirical distribution of random sample means\"\"\"\n",
    "\n",
    "def simulate_sample_mean(table, label, sample_size, repetitions):\n",
    "    \n",
    "    means = []\n",
    "\n",
    "    for i in np.arange(repetitions):\n",
    "        new_sample = ...\n",
    "        new_sample_mean = ...\n",
    "        ...\n",
    "\n",
    "    sample_means = Table().with_column('Sample Means', means)\n",
    "    \n",
    "    # Display empirical histogram and print all relevant quantities – don't change this!\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.hist(means, bins=20)\n",
    "    plt.xlabel('Sample Means')\n",
    "    plt.title('Sample Size ' + str(sample_size))\n",
    "    \n",
    "    textstr = '\\n'.join((\n",
    "    r'$\\mathrm{Sample  Size}=%.2f$' % (sample_size, ),\n",
    "    r'$\\mathrm{Population  Mean}=%.2f$' % (np.mean(table.column(label)), ),\n",
    "    r'$\\mathrm{Average Of Sample Means}=%.2f$' % (np.mean(means), ), \n",
    "    r'$\\mathrm{Population SD}=%.2f$' % (np.std(table.column(label)), ),\n",
    "    r'$\\mathrm{SD Of Sample Means}=%.2f$' % (np.std(means), )))\n",
    "\n",
    "    # these are matplotlib.patch.Patch properties\n",
    "    props = dict(boxstyle='round', facecolor='wheat', alpha=0.5)\n",
    "\n",
    "    # place a text box in upper left in axes coords\n",
    "    ax.text(0.95, 0.75, textstr, transform=ax.transAxes, fontsize=14,\n",
    "            verticalalignment='top', bbox=props)\n",
    "    \n",
    "    return means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check('tests/q8.py')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 9**\n",
    "<br/>\n",
    "In the following cell, we will create a sample of size 100 from the salaries table and graph it using our new `simulate_sample_mean` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_sample_size = simulate_sample_mean(salaries, 'salary', 100, 10000) \n",
    "plt.xlim(50000, 100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following two cells, simulate the mean of a random sample of 400 salaries and 625 salaries, respectively. In each case, perform 10,000 repetitions of each of these processes. Don't worry about the `plots.xlim` line – it just makes sure that all of the plots have the same x-axis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "for_assignment_type": "student"
   },
   "outputs": [],
   "source": [
    "...\n",
    "plt.xlim(50000, 100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "for_assignment_type": "student"
   },
   "outputs": [],
   "source": [
    "...\n",
    "plt.xlim(50000, 100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the Central Limit Theorem in action – the histograms of the sample means are roughly normal, even though the histogram of the salaries themselves is far from normal.\n",
    "\n",
    "We can also see that each of the three histograms of the sample means is centered very close to the population mean. In each case, the \"average of sample means\" is very close to the population mean. Both values are provided in the printout above each histogram. As expected, the sample mean is an unbiased estimate of the population mean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 10**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we'll look at what happens when we take a fixed sample, then bootstrap from it with different numbers of resamples. How does the distribution of the resampled means change?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulate_sample_mean(salaries, 'salary', 100, 500)\n",
    "plt.xlim(50000, 100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulate_sample_mean(salaries, 'salary', 100, 1000)\n",
    "plt.xlim(50000, 100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulate_sample_mean(salaries, 'salary', 100, 5000)\n",
    "plt.xlim(50000, 100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulate_sample_mean(salaries, 'salary', 100, 10000)\n",
    "plt.xlim(50000, 100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assign the variable `bootstrap_sampled_SD` to the integer corresponding to your answer to the following prediction question:\n",
    "\n",
    "When I increase the number of bootstrap samples that I take, for a fixed sample size, the SD of my sample mean will...\n",
    "\n",
    "1. Increase\n",
    "2. Decrease\n",
    "3. Stay about the same\n",
    "4. Vary widly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bootstrap_sampled_SD = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check('tests/q10.py')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What did you notice about the sample means of the four bootstrapped samples above?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 11**\n",
    "<br/>\n",
    "Next, let's think about how the relationships between population SD, sample SD, and SD of sample means change with varying sample size. Which of the following is true? Again, assign the variable `pop_vs_sample` to the integer corresponding to your answer. To gain some intuition, you can run the simulation cells below.\n",
    "\n",
    "1. Sample SD gets smaller with increasing sample size, SD of sample means gets smaller with increasing sample size\n",
    "2. Sample SD gets larger with increasing sample size, SD of sample means stays the same with increasing sample size\n",
    "3. Sample SD becomes more consistent with population SD with increasing sample size, SD of sample means gets smaller with increasing sample size\n",
    "4. Sample SD becomes more consistent with populatoin SD with increasing sample size, SD of smaple means stays the same with increasing sample size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_vs_sample = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check('tests/q11.py')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what happens: First, we calculate the population SD so that we can compare the SD of each sample to the SD of the population."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_sd = np.std(salaries.column(\"salary\"))\n",
    "pop_sd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's then how a small sample behaves. Run the following cells multiple times to see how the SD of the sample changes from sample to sample. Adjust the bins as necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_10 = salaries.sample(10)\n",
    "sample_10.hist(\"salary\")\n",
    "print(\"Sample SD: \", np.std(sample_10.column(\"salary\")))\n",
    "simulate_sample_mean(sample_10, 'salary', 10, 1000)\n",
    "plt.xlim(5,120000)\n",
    "plt.ylim(0, .0001);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_200 = salaries.sample(200)\n",
    "sample_200.hist(\"salary\")\n",
    "print(\"Sample SD: \", np.std(sample_200.column(\"salary\")))\n",
    "simulate_sample_mean(sample_200, 'salary', 200, 1000)\n",
    "plt.xlim(5,100000)\n",
    "plt.ylim(0, .00015);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_1000 = salaries.sample(1000)\n",
    "sample_1000.hist(\"salary\")\n",
    "print(\"Sample SD: \", np.std(sample_1000.column(\"salary\")))\n",
    "simulate_sample_mean(sample_1000, 'salary', 1000, 1000)\n",
    "plt.xlim(5,100000)\n",
    "plt.ylim(0, .00025);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's illustrate this trend. Below, you will see how the average absolute error of SD from the population changes with sample size (N)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't change this cell, just run it!\n",
    "sample_n_errors = make_array()\n",
    "for i in np.arange(10, 200, 10):\n",
    "    sample_n_errors = np.append(sample_n_errors, np.average([abs(np.std(salaries.sample(i).column(\"salary\"))-pop_sd)\n",
    "                                                             for d in np.arange(100)]))\n",
    "Table().with_columns(\"Average absolute error in SD\", sample_n_errors, \"N\", np.arange(10, 200, 10)).plot(\"N\", \"Average absolute error in SD\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should notice that the distribution of means gets spiker, and that the distribution of the sample increasingly looks like the distribution of the population as we get to larger sample sizes. \n",
    "\n",
    "Is there a relationship between the sample size and absolute error in standard deviation? Identify this relationship – if you're having trouble, take a look at this [section](https://inferentialthinking.com/chapters/14/5/Variability_of_the_Sample_Mean.html) in our textbook about the variability of sample means."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You're finished with lab 8! In order to successfully submit your assignment, follow these steps...\n",
    "- **IMPORTANT** Before you do anything, **Save and Checkpoint** from the `File` menu. Please do this first before running the cell below,\n",
    "- **run all the tests and verify that they all pass** (the next cell has a shortcut for that), \n",
    "- **Review the notebook one last time** If you make any changes, please **Save and Checkpoint** again.\n",
    "- Download as an .html file and/or an .ipynb file and submit on Canvas! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For your convenience, you can run this cell to run all the tests at once!\n",
    "import glob\n",
    "from gofer.ok import check\n",
    "correct = 0\n",
    "checks = [1,3,5,6,7,8,10,11]\n",
    "total = len(checks)\n",
    "for x in checks:\n",
    "    print('Testing question {}: '.format(str(x)))\n",
    "    g = check('tests/q{}.py'.format(str(x)))\n",
    "    if g.grade == 1.0:\n",
    "        print(\"Passed\")\n",
    "        correct += 1\n",
    "    else:\n",
    "        print('Failed')\n",
    "        display(g)\n",
    "\n",
    "print('Grade:  {}'.format(str(correct/total)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "course": "8x",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "lab": "lab01",
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "section": "3"
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
